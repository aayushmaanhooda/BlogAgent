{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10aeb6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from __future__ import annotations\n",
    "\n",
    "import operator\n",
    "from typing import TypedDict, List, Annotated\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph import StateGraph, START , END\n",
    "from langgraph.types import Send\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43f007b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task(BaseModel):\n",
    "    id: int\n",
    "    title: str\n",
    "    brief: str = Field(..., description=\"What to cover\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cabc0e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plan(BaseModel):\n",
    "    blog_title: str\n",
    "    tasks: List[Task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fc1a9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    plan: Plan\n",
    "    # reducer: results from workers get concatenated automatically\n",
    "    sections: Annotated[List[str], operator.add]\n",
    "    final: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaa1e054",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_chat_model(\"gpt-4.1-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ef72b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# orchestrator node\n",
    "\n",
    "def orchestrator(state: State) -> dict:\n",
    "    plan = llm.with_structured_output(Plan).invoke(\n",
    "        [\n",
    "            SystemMessage(content=(\"Create a blog with 5-7 sections on the following topic\")),\n",
    "            HumanMessage(content=f\"Topic: {state['topic']}\"),\n",
    "        ]\n",
    "    )\n",
    "    return {\n",
    "        \"plan\": plan \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce2af0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The `Send` class is used within a `StateGraph`'s conditional edges to\n",
    "    dynamically invoke a node with a custom state at the next step.\n",
    "\n",
    "    Importantly, the sent state can differ from the core graph's state,\n",
    "    allowing for flexible and dynamic workflow management.\n",
    "\n",
    "    Send is LangGraph's way of saying \"spawn a new parallel execution to this node with this specific data.\"\n",
    "    \n",
    "    ## Visual Representation\n",
    "\n",
    "                    ┌─── Send(\"Worker\", {task_1, topic, plan}) ──► Worker\n",
    "                    │\n",
    "                    ├─── Send(\"Worker\", {task_2, topic, plan}) ──► Worker\n",
    "                    │\n",
    "fanout() returns ───┼─── Send(\"Worker\", {task_3, topic, plan}) ──► Worker\n",
    "                    │\n",
    "                    ├─── Send(\"Worker\", {task_4, topic, plan}) ──► Worker\n",
    "                    │\n",
    "                    └─── Send(\"Worker\", {task_5, topic, plan}) ──► Worker\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "def fanout(state: State):\n",
    "    return [\n",
    "        Send(\"Worker\", {\"task\":task, \"topic\": state[\"topic\"], \"plan\": state[\"plan\"]})\n",
    "        for task in state[\"plan\"].tasks\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e7b48a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(payload:dict) -> dict:\n",
    "    task = payload[\"task\"]\n",
    "    topic= payload[\"topic\"]\n",
    "    plan =payload[\"plan\"]\n",
    "\n",
    "    blog_title = plan.blog_title\n",
    "\n",
    "    section_md = llm.invoke(\n",
    "        [\n",
    "            SystemMessage(content=\"Write one clean Markdown Section\"),\n",
    "            HumanMessage(content=(\n",
    "                f\"Blog: {blog_title}\\n\",\n",
    "                f\"Topic: {topic}\\n\",\n",
    "                f\"Section: {task.title}\\n\",\n",
    "                f\"Brief: {task.brief}\\n\\n\",\n",
    "                \"Return only the section content in Markdown\"\n",
    "            ))\n",
    "        ]\n",
    "    ).content.strip()\n",
    "\n",
    "    return {\n",
    "        \"sections\": [section_md]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7b2da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def reducer(state: State) -> dict:\n",
    "    title = state[\"plan\"].blog_title\n",
    "    body = \"\\n\\n\".join(state[\"sections\"]).strip()\n",
    "\n",
    "    final_md = f\"# {title}\\n\\n{body}\\n\"\n",
    "\n",
    "    # save to file \n",
    "\n",
    "    filename = title.lower().replace(\" \", \"_\") + \".md\"\n",
    "    output_path = Path(filename)\n",
    "    output_path.write_text(final_md, encoding=\"utf-8\")\n",
    "\n",
    "    return {\n",
    "        \"final\" : final_md\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
