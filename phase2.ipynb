{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f922eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import operator\n",
    "from pathlib import Path\n",
    "from typing import TypedDict, List, Annotated, Literal\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Send\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e4c717a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task(BaseModel):\n",
    "    id: int\n",
    "    title: str\n",
    "\n",
    "    goal: str = Field(\n",
    "        ...,\n",
    "        description=\"One sentence describing what the reader should be able to do/understand after this section.\",\n",
    "    )\n",
    "    bullets: List[str] = Field(\n",
    "        ...,\n",
    "        min_length=3,\n",
    "        max_length=5,\n",
    "        description=\"3–5 concrete, non-overlapping subpoints to cover in this section.\",\n",
    "    )\n",
    "    target_words: int = Field(\n",
    "        ...,\n",
    "        description=\"Target word count for this section (120–450).\",\n",
    "    )\n",
    "    section_type: Literal[\n",
    "        \"intro\", \"core\", \"examples\", \"checklist\", \"common_mistakes\", \"conclusion\"\n",
    "    ] = Field(\n",
    "        ...,\n",
    "        description=\"Use 'common_mistakes' exactly once in the plan.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85bf5667",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plan(BaseModel):\n",
    "    blog_title: str\n",
    "    audience: str = Field(..., description=\"Who this blog is for.\")\n",
    "    tone: str = Field(..., description=\"Writing tone (e.g., practical, crisp).\")\n",
    "    tasks: List[Task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6615cd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    plan: Plan\n",
    "    sections: Annotated[List[str], operator.add]  # reducer concatenates worker outputs\n",
    "    final: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d910f668",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_chat_model(\"gpt-4.1-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d6c8c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def orchestrator(state: State) -> dict:\n",
    "    planner = llm.with_structured_output(Plan)\n",
    "\n",
    "    plan = planner.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=(\n",
    "                    \"You are a senior technical writer and developer advocate. Your job is to produce a \"\n",
    "                    \"highly actionable outline for a technical blog post.\\n\\n\"\n",
    "                    \"Hard requirements:\\n\"\n",
    "                    \"- Create 5–7 sections (tasks) that fit a technical blog.\\n\"\n",
    "                    \"- Each section must include:\\n\"\n",
    "                    \"  1) goal (1 sentence: what the reader can do/understand after the section)\\n\"\n",
    "                    \"  2) 3–5 bullets that are concrete, specific, and non-overlapping\\n\"\n",
    "                    \"  3) target word count (120–450)\\n\"\n",
    "                    \"- Include EXACTLY ONE section with section_type='common_mistakes'.\\n\\n\"\n",
    "                    \"Make it technical (not generic):\\n\"\n",
    "                    \"- Assume the reader is a developer; use correct terminology.\\n\"\n",
    "                    \"- Prefer design/engineering structure: problem → intuition → approach → implementation → \"\n",
    "                    \"trade-offs → testing/observability → conclusion.\\n\"\n",
    "                    \"- Bullets must be actionable and testable (e.g., 'Show a minimal code snippet for X', \"\n",
    "                    \"'Explain why Y fails under Z condition', 'Add a checklist for production readiness').\\n\"\n",
    "                    \"- Explicitly include at least ONE of the following somewhere in the plan (as bullets):\\n\"\n",
    "                    \"  * a minimal working example (MWE) or code sketch\\n\"\n",
    "                    \"  * edge cases / failure modes\\n\"\n",
    "                    \"  * performance/cost considerations\\n\"\n",
    "                    \"  * security/privacy considerations (if relevant)\\n\"\n",
    "                    \"  * debugging tips / observability (logs, metrics, traces)\\n\"\n",
    "                    \"- Avoid vague bullets like 'Explain X' or 'Discuss Y'. Every bullet should state what \"\n",
    "                    \"to build/compare/measure/verify.\\n\\n\"\n",
    "                    \"Ordering guidance:\\n\"\n",
    "                    \"- Start with a crisp intro and problem framing.\\n\"\n",
    "                    \"- Build core concepts before advanced details.\\n\"\n",
    "                    \"- Include one section for common mistakes and how to avoid them.\\n\"\n",
    "                    \"- End with a practical summary/checklist and next steps.\\n\\n\"\n",
    "                    \"Output must strictly match the Plan schema.\"\n",
    "                )\n",
    "            ),\n",
    "            HumanMessage(content=f\"Topic: {state['topic']}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"plan\": plan}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6d6a43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fanout(state: State):\n",
    "    return [\n",
    "        Send(\n",
    "            \"worker\",\n",
    "            {\"task\": task, \"topic\": state[\"topic\"], \"plan\": state[\"plan\"]},\n",
    "        )\n",
    "        for task in state[\"plan\"].tasks\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15ca6151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(payload: dict) -> dict:\n",
    "\n",
    "    task = payload[\"task\"]\n",
    "    topic = payload[\"topic\"]\n",
    "    plan = payload[\"plan\"]\n",
    "\n",
    "    bullets_text = \"\\n- \" + \"\\n- \".join(task.bullets)\n",
    "\n",
    "    section_md = llm.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "    content=(\n",
    "        \"You are a senior technical writer and developer advocate. Write ONE section of a technical blog post in Markdown.\\n\\n\"\n",
    "        \"Hard constraints:\\n\"\n",
    "        \"- Follow the provided Goal and cover ALL Bullets in order (do not skip or merge bullets).\\n\"\n",
    "        \"- Stay close to the Target words (±15%).\\n\"\n",
    "        \"- Output ONLY the section content in Markdown (no blog title H1, no extra commentary).\\n\\n\"\n",
    "        \"Technical quality bar:\\n\"\n",
    "        \"- Be precise and implementation-oriented (developers should be able to apply it).\\n\"\n",
    "        \"- Prefer concrete details over abstractions: APIs, data structures, protocols, and exact terms.\\n\"\n",
    "        \"- When relevant, include at least one of:\\n\"\n",
    "        \"  * a small code snippet (minimal, correct, and idiomatic)\\n\"\n",
    "        \"  * a tiny example input/output\\n\"\n",
    "        \"  * a checklist of steps\\n\"\n",
    "        \"  * a diagram described in text (e.g., 'Flow: A -> B -> C')\\n\"\n",
    "        \"- Explain trade-offs briefly (performance, cost, complexity, reliability).\\n\"\n",
    "        \"- Call out edge cases / failure modes and what to do about them.\\n\"\n",
    "        \"- If you mention a best practice, add the 'why' in one sentence.\\n\\n\"\n",
    "        \"Markdown style:\\n\"\n",
    "        \"- Start with a '## <Section Title>' heading.\\n\"\n",
    "        \"- Use short paragraphs, bullet lists where helpful, and code fences for code.\\n\"\n",
    "        \"- Avoid fluff. Avoid marketing language.\\n\"\n",
    "        \"- If you include code, keep it focused on the bullet being addressed.\\n\"\n",
    "    )\n",
    ")\n",
    ",\n",
    "            HumanMessage(\n",
    "                content=(\n",
    "                    f\"Blog: {plan.blog_title}\\n\"\n",
    "                    f\"Audience: {plan.audience}\\n\"\n",
    "                    f\"Tone: {plan.tone}\\n\"\n",
    "                    f\"Topic: {topic}\\n\\n\"\n",
    "                    f\"Section: {task.title}\\n\"\n",
    "                    f\"Section type: {task.section_type}\\n\"\n",
    "                    f\"Goal: {task.goal}\\n\"\n",
    "                    f\"Target words: {task.target_words}\\n\"\n",
    "                    f\"Bullets:{bullets_text}\\n\"\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "    ).content.strip()\n",
    "\n",
    "    return {\"sections\": [section_md]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87d4028c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducer(state: State) -> dict:\n",
    "\n",
    "    title = state[\"plan\"].blog_title\n",
    "    body = \"\\n\\n\".join(state[\"sections\"]).strip()\n",
    "    \n",
    "    final_md = f\"# {title}\\n\\n{body}\\n\"\n",
    "\n",
    "    # Save to file\n",
    "    filename = \"\".join(c if c.isalnum() or c in (\" \", \"_\", \"-\") else \"\" for c in title)\n",
    "    filename = filename.strip().lower().replace(\" \", \"_\") + \".md\"\n",
    "    Path(f\"Blogs/{filename}\").write_text(final_md, encoding=\"utf-8\")\n",
    "\n",
    "    return {\"final\": final_md}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f374164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x110587a10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = StateGraph(State)\n",
    "g.add_node(\"orchestrator\", orchestrator)\n",
    "g.add_node(\"worker\", worker)\n",
    "g.add_node(\"reducer\", reducer)\n",
    "\n",
    "g.add_edge(START, \"orchestrator\")\n",
    "g.add_conditional_edges(\"orchestrator\", fanout, [\"worker\"])\n",
    "g.add_edge(\"worker\", \"reducer\")\n",
    "g.add_edge(\"reducer\", END)\n",
    "\n",
    "app = g.compile()\n",
    "\n",
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f90f9407",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = app.invoke({\"topic\": \"Write a blog on Self Attention\", \"sections\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "758e2767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Mastering Self-Attention: Concepts, Implementation, and Best Practices\n",
      "\n",
      "## Introduction to Self-Attention Mechanism\n",
      "\n",
      "Self-attention is a mechanism that enables a model to weigh the importance of different elements within a single input sequence when encoding information. Unlike traditional attention, which relates two distinct sequences (e.g., query and key/value from separate sources), self-attention computes attention scores *within* the same sequence. This allows the model to dynamically capture contextual dependencies regardless of their distance in the input.\n",
      "\n",
      "The key problem self-attention addresses is efficiently modeling relationships between all pairs of tokens in an input sequence. Classic sequential models like RNNs suffer from difficulty capturing long-range dependencies due to vanishing gradients and sequential processing. Self-attention solves this by enabling direct connections between any two positions, allowing simultaneous consideration of full sequence context in parallel.\n",
      "\n",
      "Intuitively, self-attention assigns a weighted importance to each token relative to others. Each element attends to all tokens by computing similarity scores (often dot-product based) to determine how much that token should influence the current token’s representation. This weighted aggregation captures nuanced relationships such as syntax in language or spatial dependencies in images.\n",
      "\n",
      "Self-attention underpins Transformer architectures widely used in Natural Language Processing (e.g., BERT, GPT) and increasingly in Computer Vision (e.g., Vision Transformers). Its ability to model global context and scale efficiently with parallel hardware makes it a foundational building block in modern deep learning.\n",
      "\n",
      "Architecturally, self-attention is implemented in a transformer block where the input embeddings are projected into query, key, and value vectors. Attention weights are computed by scaling dot products of queries and keys, normalized with softmax, and used to aggregate values. This output then passes through feed-forward layers and normalization, creating a powerful representation that encodes relationships across the entire input sequence.\n",
      "\n",
      "Flow:\n",
      "```\n",
      "Input Embeddings -> Linear Projections (Q, K, V) -> Scaled Dot-Product Attention -> Weighted Sum of V -> Output Representation\n",
      "```\n",
      "\n",
      "## Core Concepts and Mathematical Formulation of Self-Attention\n",
      "\n",
      "Self-attention operates by computing a set of attention weights that express how much each element in the input sequence should attend to every other element. This requires transforming the input embeddings into three distinct vectors for each token in the sequence: queries, keys, and values.\n",
      "\n",
      "### Query, Key, and Value Vectors Derivation\n",
      "\n",
      "Given an input sequence represented as matrix \\( X \\in \\mathbb{R}^{n \\times d} \\) where \\( n \\) is the sequence length and \\( d \\) the embedding dimension, the query (\\( Q \\)), key (\\( K \\)), and value (\\( V \\)) matrices are computed as linear projections:\n",
      "\n",
      "\\[\n",
      "Q = XW^Q, \\quad K = XW^K, \\quad V = XW^V\n",
      "\\]\n",
      "\n",
      "Here, \\( W^Q, W^K, W^V \\in \\mathbb{R}^{d \\times d_k} \\) are learned parameter matrices and \\( d_k \\) is the dimension of the key/query vectors, often equal for simplicity. These projections allow the model to represent queries, keys, and values in separate subspaces optimized for the attention mechanism.\n",
      "\n",
      "### Scaled Dot-Product Attention Formula\n",
      "\n",
      "Self-attention scores are calculated by computing the dot products of queries with all keys:\n",
      "\n",
      "\\[\n",
      "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) V\n",
      "\\]\n",
      "\n",
      "Breaking down this formula step-by-step:\n",
      "\n",
      "1. Compute the score matrix \\( S = QK^\\top \\), measuring the similarity between each query and all keys.\n",
      "2. Scale the scores by \\( \\frac{1}{\\sqrt{d_k}} \\) to prevent large dot-product magnitudes destabilizing gradients.\n",
      "3. Apply softmax normalization along each row of the scaled scores to convert them into a valid probability distribution:\n",
      "   \\[\n",
      "   \\alpha_{ij} = \\frac{e^{s_{ij}/\\sqrt{d_k}}}{\\sum_{j} e^{s_{ij}/\\sqrt{d_k}}}\n",
      "   \\]\n",
      "   where \\( \\alpha_{ij} \\) indicates how much the \\( i \\)-th query attends to the \\( j \\)-th key.\n",
      "4. Use the attention weights \\( \\alpha \\) to compute a weighted sum over the values \\( V \\) producing the output.\n",
      "\n",
      "### Minimal Self-Attention Example in Python with NumPy\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "# Input: 3 tokens, embedding dimension 4\n",
      "X = np.array([[1, 0, 1, 0],\n",
      "              [0, 2, 0, 2],\n",
      "              [1, 1, 1, 1]], dtype=float)\n",
      "\n",
      "# Randomly initialized weights mapping input dim 4 -> d_k = 2\n",
      "Wq = np.array([[0.2, 0.4],\n",
      "               [0.5, 0.1],\n",
      "               [0.3, 0.7],\n",
      "               [0.6, 0.9]])\n",
      "Wk = np.array([[0.5, 0.3],\n",
      "               [0.8, 0.2],\n",
      "               [0.6, 0.1],\n",
      "               [0.9, 0.4]])\n",
      "Wv = np.array([[0.1, 0.7],\n",
      "               [0.3, 0.8],\n",
      "               [0.5, 0.2],\n",
      "               [0.9, 0.4]])\n",
      "\n",
      "# Compute Q, K, V\n",
      "Q = X.dot(Wq)  # shape (3, 2)\n",
      "K = X.dot(Wk)  # shape (3, 2)\n",
      "V = X.dot(Wv)  # shape (3, 2)\n",
      "\n",
      "# Dot product attention scores, scaled\n",
      "d_k = Q.shape[1]\n",
      "scores = Q.dot(K.T) / np.sqrt(d_k)\n",
      "\n",
      "# Softmax along rows\n",
      "exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n",
      "attention_weights = exp_scores / exp_scores.sum(axis=1, keepdims=True)\n",
      "\n",
      "# Weighted sum\n",
      "output = attention_weights.dot(V)\n",
      "\n",
      "print(\"Attention Weights:\\n\", attention_weights)\n",
      "print(\"Output:\\n\", output)\n",
      "```\n",
      "\n",
      "### Importance of Scaling by \\(\\sqrt{d_k}\\)\n",
      "\n",
      "Scaling the dot-product by \\(\\frac{1}{\\sqrt{d_k}}\\) is critical because the variance of dot products grows with the dimensionality \\(d_k\\). Without this scaling, large values saturate the softmax, causing gradients to vanish during backpropagation. Dividing by \\(\\sqrt{d_k}\\) keeps the score magnitudes in a sensible range, ensuring stable gradient flow and better convergence during training.\n",
      "\n",
      "### Role of Softmax Normalization\n",
      "\n",
      "Softmax converts raw attention scores into a probability distribution over the keys for each query. This normalization enforces that weights are positive and sum to 1, making the weighted average of values meaningful. The softmax sharpens or flattens the attention distribution, dynamically controlling focus between broader context or specialized tokens depending on learned representations. This probabilistic interpretation is essential for interpretability and gradient-based optimization.\n",
      "\n",
      "## Implementing a Simple Self-Attention Layer from Scratch\n",
      "\n",
      "Below is a PyTorch implementation of a single self-attention layer. It includes learnable linear projections for queries, keys, and values, processes a batch of token embeddings, and produces attended outputs of the same shape.\n",
      "\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "\n",
      "class SelfAttention(nn.Module):\n",
      "    def __init__(self, embed_dim):\n",
      "        super(SelfAttention, self).__init__()\n",
      "        self.embed_dim = embed_dim\n",
      "        # Linear projections for query, key, value\n",
      "        self.query_proj = nn.Linear(embed_dim, embed_dim)\n",
      "        self.key_proj = nn.Linear(embed_dim, embed_dim)\n",
      "        self.value_proj = nn.Linear(embed_dim, embed_dim)\n",
      "        # Scaling factor for stability (sqrt of embed_dim)\n",
      "        self.scale = embed_dim ** 0.5\n",
      "\n",
      "    def forward(self, x):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            x: Tensor of shape (batch_size, seq_len, embed_dim)\n",
      "        Returns:\n",
      "            attended_outputs: Tensor of shape (batch_size, seq_len, embed_dim)\n",
      "        \"\"\"\n",
      "        # Project input embeddings to queries, keys, values\n",
      "        Q = self.query_proj(x)  # (B, N, E)\n",
      "        K = self.key_proj(x)    # (B, N, E)\n",
      "        V = self.value_proj(x)  # (B, N, E)\n",
      "\n",
      "        # Compute attention scores: Q @ K^T\n",
      "        # K transpose shape: (B, E, N) to match for batch matrix multiplication\n",
      "        scores = torch.bmm(Q, K.transpose(1, 2))  # (B, N, N)\n",
      "\n",
      "        # Scale scores to stabilize gradients\n",
      "        scores = scores / self.scale\n",
      "\n",
      "        # Softmax to get attention weights\n",
      "        attn_weights = F.softmax(scores, dim=-1)  # (B, N, N)\n",
      "\n",
      "        # Weighted sum of values according to attention weights\n",
      "        attended_outputs = torch.bmm(attn_weights, V)  # (B, N, E)\n",
      "\n",
      "        return attended_outputs, attn_weights\n",
      "```\n",
      "\n",
      "### Using the Self-Attention Layer\n",
      "\n",
      "```python\n",
      "batch_size = 2\n",
      "seq_len = 4\n",
      "embed_dim = 8\n",
      "\n",
      "# Example input: batch of token embeddings\n",
      "x = torch.rand(batch_size, seq_len, embed_dim)\n",
      "self_attn = SelfAttention(embed_dim)\n",
      "\n",
      "out, attn_weights = self_attn(x)\n",
      "print(out.shape)        # Expected: (2, 4, 8)\n",
      "print(attn_weights.shape) # Expected: (2, 4, 4)\n",
      "```\n",
      "\n",
      "- Input `x` shape: `(batch_size, sequence_length, embedding_dim)`\n",
      "- `Q`, `K`, `V` have the same shape `(B, N, E)`\n",
      "- Attention scores computed as batch matrix multiplication: `(B, N, E) x (B, E, N) -> (B, N, N)`\n",
      "- Softmax applied along last axis over the key dimension\n",
      "- Final output combines values weighted by attention scores\n",
      "\n",
      "### Performance Considerations\n",
      "\n",
      "- Time and memory complexity scale **quadratically** with sequence length `N` (`O(N²)`) due to the attention score matrix `(N x N)`.\n",
      "- This complexity limits the direct use of self-attention for very long sequences (e.g., thousands of tokens).\n",
      "- Possible optimizations:\n",
      "  - **Sparse attention**: restrict attention to local windows or predefined patterns to reduce computations.\n",
      "  - **Low-rank / kernel approximations**: approximate attention matrices to avoid full `N x N` cost.\n",
      "  - **Chunking or memory-compressed attention**: process input in smaller segments or compress keys and values.\n",
      "- Trade-off: optimizations may reduce expressive capacity or introduce additional parameters.\n",
      "\n",
      "### Debugging Tips\n",
      "\n",
      "- Check intermediate tensor shapes after each step (`Q`, `K`, `V`, scores, attention weights) to ensure correct broadcasting and batching.\n",
      "- Validate that attention weights sum to 1 along the key dimension:\n",
      "\n",
      "```python\n",
      "assert torch.allclose(attn_weights.sum(dim=-1), torch.ones(batch_size, seq_len), atol=1e-6)\n",
      "```\n",
      "\n",
      "- Visualize attention matrices per sample or head (if multi-head):\n",
      "\n",
      "```python\n",
      "import matplotlib.pyplot as plt\n",
      "plt.imshow(attn_weights[0].detach().cpu())\n",
      "plt.title(\"Attention Weights Heatmap\")\n",
      "plt.colorbar()\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "- Inspect attention patterns to confirm meaningful token interactions (e.g., local attention for language data).\n",
      "- If outputs contain NaNs or exploding values, check scaling factor usage and input normalization.\n",
      "\n",
      "This implementation provides a minimal yet complete self-attention module suitable for integration into larger transformer models or experimental architectures.\n",
      "\n",
      "## Common Mistakes and Pitfalls When Working with Self-Attention\n",
      "\n",
      "When implementing self-attention, developers often face subtle errors that can break training or degrade model performance. Here are frequent pitfalls to watch out for, along with their fixes:\n",
      "\n",
      "- **Incorrect tensor dimension alignments**  \n",
      "Self-attention relies on key, query, and value tensors shaped `[batch_size, seq_len, dim]`. Matrix multiplications like `Q * Kᵀ` require careful dimension ordering and reshaping to avoid broadcasting errors. For example, when computing attention scores:  \n",
      "```python\n",
      "# Q: [B, T, D], K: [B, T, D]\n",
      "scores = torch.matmul(Q, K.transpose(-2, -1))  # Result: [B, T, T]\n",
      "```\n",
      "Always verify transpose axes and batch dimensions. Mismatched shapes can cause runtime errors or silent bugs if PyTorch broadcasts unexpectedly.\n",
      "\n",
      "- **Neglecting the scaling factor**  \n",
      "The dot-product in self-attention grows with the dimension size `d_k`, which can lead to large values and gradient saturation. Scaling attention scores by `1/√d_k` before softmax stabilizes gradients:  \n",
      "```python\n",
      "scores = scores / math.sqrt(d_k)\n",
      "```\n",
      "Forget this step and you risk unstable training or slow convergence.\n",
      "\n",
      "- **Failing to mask padding tokens**  \n",
      "For variable-length sequences padded to a fixed size, unmasked padding positions will receive attention weights, contaminating the representation. Always apply a mask before softmax:  \n",
      "```python\n",
      "scores = scores.masked_fill(pad_mask == 0, float('-inf'))\n",
      "attn = torch.softmax(scores, dim=-1)\n",
      "```\n",
      "This prevents the model focusing on meaningless padding tokens, improving accuracy.\n",
      "\n",
      "- **Ignoring computational cost on long sequences**  \n",
      "Self-attention computes all pairwise interactions, leading to O(T²) complexity. This exhausts GPU memory with long sequences. Mitigate by:  \n",
      "  * Using truncated context windows  \n",
      "  * Applying sparse or local attention variants  \n",
      "  * Reducing batch size or sequence length  \n",
      "Careful profiling is necessary to balance memory use and model capacity.\n",
      "\n",
      "- **Overlooking numerical instability in softmax**  \n",
      "Large or small values in attention scores cause overflow/underflow in softmax calculations. Use the log-sum-exp trick for stability:  \n",
      "```python\n",
      "max_score, _ = scores.max(dim=-1, keepdim=True)\n",
      "scores = scores - max_score\n",
      "attn = torch.softmax(scores, dim=-1)\n",
      "```\n",
      "This simple shift prevents NaNs and ensures more robust training.\n",
      "\n",
      "**Summary checklist:**  \n",
      "- Verify tensor shapes and transpose axes carefully  \n",
      "- Apply the `1/√d_k` scaling on scores before softmax  \n",
      "- Mask padded tokens to exclude them from attention  \n",
      "- Monitor memory usage; consider optimized attention for long inputs  \n",
      "- Use numerically stable softmax computations with the log-sum-exp trick  \n",
      "\n",
      "Addressing these pitfalls improves both model stability and performance when working with self-attention.\n",
      "\n",
      "## Advanced Topics: Performance and Scalability of Self-Attention\n",
      "\n",
      "Vanilla self-attention computes pairwise interactions between all tokens in a sequence, resulting in **O(N²)** time and memory complexity, where N is the sequence length. This quadratic bottleneck makes scaling to long sequences (10k+ tokens) prohibitively expensive in production models, leading to slow inference and high memory consumption.\n",
      "\n",
      "### Sparse and Local Windowed Attention\n",
      "\n",
      "To mitigate this, **sparse attention** restricts interactions to a subset of token pairs based on predefined patterns or learned sparsity. For example:\n",
      "\n",
      "- **Local window attention** attends only to tokens within a fixed-size neighborhood, e.g., ±k tokens around each position. This lowers complexity to **O(Nk)** and suits tasks with strong local dependencies such as speech or time series.\n",
      "- **Strided and dilated attention** attend every s-th token to capture broader context without full self-attention.\n",
      "- **Block sparse attention**, where attention is computed over blocks of tokens rather than individuals (e.g., BigBird), enables scaling to very long documents.\n",
      "\n",
      "Practical use cases:\n",
      "- Transformers in language modeling with long paragraphs.\n",
      "- Vision transformers processing high-resolution images via localized patches.\n",
      "- Audio transformers focusing on limited temporal windows.\n",
      "\n",
      "### Approximate and Kernel-Based Attention\n",
      "\n",
      "Approximate methods aim to algorithmically reduce computation with controlled approximation:\n",
      "\n",
      "- **Linformer** projects keys and values into a lower-dimensional subspace to attain linear complexity.\n",
      "- **Performer** applies kernel methods to approximate the softmax attention with random feature maps.\n",
      "- **Nyströmformer** uses low-rank approximations of the attention matrix via landmark points.\n",
      "\n",
      "These methods reduce complexity to **O(N)** or **O(N log N)**, making them appealing for high-throughput or memory-constrained environments.\n",
      "\n",
      "### Trade-offs Between Approximation and Accuracy\n",
      "\n",
      "Approximate attention methods often introduce some error in attention weights:\n",
      "\n",
      "- In **quality-critical applications** like language generation or translation, small approximation errors can degrade model output fluency or accuracy.\n",
      "- Sparse attention may omit important long-range dependencies.\n",
      "- Kernel-based approximations can suffer from stability issues or require careful hyperparameter tuning.\n",
      "\n",
      "Thus, choose approximation techniques based on acceptable accuracy loss, latency requirements, and resource constraints. Benchmarking with task-specific validation is essential.\n",
      "\n",
      "### Profiling and Monitoring Attention Layers\n",
      "\n",
      "To optimize and scale attention models, integrate profiling tools that track:\n",
      "\n",
      "- **FLOPs and latency** per attention layer to identify bottlenecks.\n",
      "- **GPU/CPU memory usage** detailing peak allocation during attention computation.\n",
      "- **Cache behavior** and parallelization efficiency in multi-GPU training.\n",
      "\n",
      "Tools like NVIDIA Nsight Systems, PyTorch Profiler, or TensorBoard profiling support these metrics. Regular monitoring enables iterative optimization and early detection of performance regressions.\n",
      "\n",
      "---\n",
      "\n",
      "**Summary checklist:**\n",
      "\n",
      "- Understand O(N²) bottleneck in vanilla attention.\n",
      "- Use sparse/local attention for scalable contextual windows.\n",
      "- Apply kernel-based methods for linear-time approximations.\n",
      "- Assess trade-offs based on accuracy criticality.\n",
      "- Continuously profile to optimize runtime and memory use.\n",
      "\n",
      "By combining these techniques thoughtfully, developers can deploy self-attention models at practical scales while managing resource budgets and maintaining performance quality.\n",
      "\n",
      "## Summary, Checklist, and Next Steps for Mastering Self-Attention\n",
      "\n",
      "### Recap of Key Mathematical & Implementation Aspects\n",
      "- Self-attention computes attention weights using scaled dot-product:  \n",
      "  \\( \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) V \\)  \n",
      "  where \\(Q, K, V \\in \\mathbb{R}^{n \\times d_k}\\) represent query, key, and value matrices.\n",
      "- Scaling by \\(\\sqrt{d_k}\\) prevents gradient vanishing and stabilizes training.\n",
      "- Implement using batched matrix multiplications, ensuring efficient tensor shapes:  \n",
      "  typical shape flow: \\((\\text{batch}, \\text{seq\\_len}, d_{\\text{model}}) \\rightarrow Q,K,V\\) with shape \\((\\text{batch}, \\text{seq\\_len}, d_k)\\).\n",
      "- Apply masking to ignore paddings or future tokens in autoregressive tasks.\n",
      "- Use efficient frameworks (PyTorch, TensorFlow) leveraging built-in functions e.g., `torch.bmm()` or `tf.linalg.matmul()`.\n",
      "\n",
      "### Production Readiness Checklist\n",
      "- [ ] **Correct scaling** by \\( \\sqrt{d_k} \\) applied before softmax for numerical stability.\n",
      "- [ ] **Proper masking** implemented to exclude padding tokens or causal future tokens in logits.\n",
      "- [ ] **Shape validation** confirmed at every step: \\(Q, K, V\\) and output tensors have compatible dimensions.\n",
      "- [ ] **Memory profiling** performed to detect bottlenecks or large allocations, especially on long sequences.\n",
      "- [ ] **Numerical stability checks** for edge cases with large sequence lengths or batch sizes.\n",
      "\n",
      "### Next Learning Steps\n",
      "- Understand **Multi-Head Attention** to parallelize attention with multiple subspaces.\n",
      "- Explore **Positional Encoding** techniques (sinusoidal, learned) essential for sequence order information.\n",
      "- Dive into **Transformer Architectures** (e.g., Vaswani et al., BERT, GPT) for full self-attention integration.\n",
      "\n",
      "### Encouraged Experiments\n",
      "- Use **attention visualization tools** like Bertviz or custom heatmaps to interpret model focus.\n",
      "- Implement **custom modifications** such as sparse attention, relative positional bias, or efficient kernels to optimize performance or accuracy.\n",
      "\n",
      "### Resources for Further Exploration\n",
      "- Research Papers:  \n",
      "  - \"Attention Is All You Need\" (Vaswani et al., 2017)  \n",
      "  - \"BERT: Pre-training of Deep Bidirectional Transformers\" (Devlin et al., 2019)  \n",
      "  - \"Longformer: The Long-Document Transformer\" (Beltagy et al., 2020)\n",
      "- Open-Source Libraries:  \n",
      "  - [Hugging Face Transformers](https://github.com/huggingface/transformers)  \n",
      "  - [TensorFlow Addons - MultiHeadAttention](https://www.tensorflow.org/addons/api_docs/python/tfa/layers/MultiHeadAttention)  \n",
      "  - [PyTorch nn.MultiheadAttention](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html)\n",
      "\n",
      "This checklist should equip you to implement, debug, and optimize self-attention modules efficiently while preparing you to build on advanced transformer concepts.\n",
      "\n",
      "## Conclusion and Future Directions\n",
      "\n",
      "Self-attention has revolutionized deep learning by enabling models to capture complex dependencies across sequences, fundamentally transforming NLP, computer vision, and beyond. Its ability to model context dynamically has led to breakthroughs such as transformer-based language models and vision transformers, setting new state-of-the-art results across tasks.\n",
      "\n",
      "Ongoing research focuses heavily on improving the efficiency and scalability of self-attention. Variants like sparse attention, Linformer, and Performer aim to reduce quadratic complexity, making it feasible to handle longer sequences and resource-constrained environments. Concurrently, domain-specific tuning techniques adapt attention mechanisms to specialized data modalities, enhancing performance and robustness.\n",
      "\n",
      "Hardware accelerations—such as custom ASICs and optimized GPU kernels—are emerging to specifically accelerate attention operations. These advancements reduce latency and energy consumption, pushing the practicality of deploying large-scale attention models in production.\n",
      "\n",
      "We encourage developers and researchers to engage with this vibrant area by experimenting with efficient attention designs, sharing benchmarks, and contributing to open-source frameworks. Advancing self-attention is critical for the next generation of AI systems that demand both power and efficiency.\n",
      "\n",
      "Mastering self-attention positions you at the forefront of AI innovation, empowering you to build adaptable, high-performance models essential for solving tomorrow’s complex challenges.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(out[\"final\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8647ff8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
